\chapter{Backround}
\label{chap:background}
\section{Database partitioning}
Many applications of large throughput and space requirements, such as popular websites, rely on distributed infrastructure. This is also the case for the databases backing these services. Scaling databases is commonly achieved by both dividing tasks and data into separate machines  with different functions (vertical partitioning) and by creating homogeneous services out of groups of machines that split the work amongs themselves.  This second type of division requires distribution of the work among separate machines.

For each table in a distributed database, the optimal partitioning scheme is chosen depending on the kind of workload expected. For example, if two large tables are expected to be joined regularly on field A, partitioning both under the same hash on field A or under the same ranges. This allows the join to proceed in parallel without need to reshuffle one of the tables. In this case the gain is of bandwidth.  Another example partitioning used for enabling parallelism is when we store temporal data, such as log records or a stock price series. If we partition on the price attribute using ranges then only one machine will be in charge of receiving all updates at one time and can be overloaded.  On the other hand, if we choose to partition by a hash on the timestamp the write load would be spread across the whole cluster.

In distributed transaction oriented systems, the cost of a transaction that must update tables in separate machine is larger, one option to preserve consistency is especially when using a protocol such as 2PC.  The rounds of communicaton involved in these protocols make such transactions both slower and a lot more expensive in resources (and the locking needed holds up even single machine transactions).  To address this kind of system Madden and Curino propose a machine analysis of work logs that then produce an optimized set of partition choices. 

An ideal partitioning strategy helps enable linear scaling of a system. Specifically, the throughput of a system should increase by a constant every time a new node is added. This is only achieved when there is no communication across nodes and no node is overloaded (as throughput decreases upon overload). On the latency side, an ideal partitioning strategy may even improve the metrics as we add node  compared to a single system, if we allow for  parallelization of work. In this case, latency could improve as we increae the number of nodes.  This may have an slight opposite effect on throughput, on the other hand. Avoiding all communication across nodes is impossible, so any partitioning strategy aims to to both balances the load across machines over time, either in terms of work or storage, and minimizes the amount of communication across them.   Often we also allow partitioning to be mixed with some form replication. While replication is often used for fault tolerance, it can also be used for performance purposes as a way to improve query performance (queries are more likely to involve less nodes nmow) at the expense of update performance (which must now do more work and becomes distributed).  

 Whenever the workload is not totally random, we can organize the database to improve performance. One such technique is cacheing, where we store recent results because we expect recently accesed locations and potentially their neighbors to be accessed soon.  Generational garbage collection techniques are based on an assumption about the distribution of object usage lifetimes, and so on. Query optimizers in databases collect statistics on usage and sizes and explictly try to optimize query plans based on these. 

There are at least two large challenges when attempting to partition. The first is that the optimal partitioning scheme depends on the workload, and as the workload changes so does the appropriate partition.  A given partition  needs to adapt, or else its performance would degrade.   A second issue is how to store such partitionings. A hash function  only requires memory of the algorithm, and a range partition requires some table for the limits of the ranges. If we decided to partition base on other rules, we start needing to remember more code or data,  and ultimately an arbitrary partition scheme could take as much space as the data it describes. So a second challenge is to find compact representations of these schemes. This is further constrained that the structure is not static, it needs to change data is inserted, computers go down or are added,  and as the workload changes. Whenever machines are added or removed, another aspect becomes important: updates to the table must be such that they transition to another optimal setup but without having the transition cost much in resources.   Hashing, for example, can be made dynamic, but special techniques such as consistent hashing are needed to make the transition between hashes (when adding /removing machines) consume as little resources as possible.

Hash based partitioning on a table will guarantee to balance tuples and requests things evenly (no overload even as the workoad properties change), but is far from optimal as far as communication across nodes is concerned. whenever a query or update involves 2 tuples. (1/n chances of any 2 rows hitting the same machine). 

Schism \cite{schism} is an approach to improve replication and partitioning strategies.  By constructing a communication graph where the nodes are the tuples and edges imply access to the corresponding tuples in the same transaction, they can use a graph partitioner and automate partitioning /replication decisions to explicitly minimize distributed transactions (METIS) this approach. One of the distinguishing features of this appraoch  is that unlike the approach from \cite{little} it does not look at the data itself, but rather at the workload. The heuristic also is able to make decisions about whether to replicate a particular piece of data.

Another area of work is in storing the tuple location informaton to allow query routing. Schism tries to infer simple predicates on tuple attributes that express the partitioning found via workload, and some other systems use a distributed hash table as their routing layer. Tatarowicz et al show one design to help solve this problem in \cite{lookup}.  Whenever a fine grained partitioning exists, there are problems both of storage and of keeping it up to date. In a database with many attributes per table, which we can query on any attribute, they have the added problem that the lookup table is very much like an index: if they don't have a lookup table on a given attribute, then they need to scan the full set of nodes (querying all shards) in order to answer a query. As we show later, this kind of all shards query can happen in parallel but it is very costly both in terms of  bandwidth, as well as latency. The paper shows several techniques that can be used to facilitate fine grained partitioning. Some of these are used in this thesis, Hybrid lookup tables keep fine grained information to a small explicit set and use a coarse grained partitioner for the rest. Another importnat part of the analysis is the logical dependency between lookup table state and actual data state. In order to simplify the system, the lookup table state is treated as soft, and stale indices can be updated by querying each node for up to date information. By loosening the consistency requirements on the index, they can handle changes to the partitioning without too much of a penalty.

A different example of workload driven partitioning is shown in  \cite{dewitt}. In this paper they worry about the effect of partitionings on the amount   parallelism vs coordination and propose a Hybrid Range partitioning strategy. Based on this strategy, they improve both latency and throughput rather than trade them off.  Some of the analysis they use in that paper is also relevant for one of the strategies evaluated here.

\section{Graph oriented systems}

Systems specifically to process or store graph data (vertices and edges and their metadata) have become more popular as the web and  social networks capture the public's attention. Their uses go beyond this sort of data, geographic locations as well as graphical models in machine learning and semantic web all thought and processed as  graph structures. A graph can be stored in a typical relational database as two tables 'vertexid' and 'edges (vertex, vertex)'.  Then, operations such as neighbors, degree per node, n-hop neighbors can be expressed with typical SQL statements. SQL is general purpose, but in this case limitations to compute things such as transitive closure and other graph concepts may motivate the development of limited but effective graph systems.

The graph processing systems described can be split broadly into two categories, some are for graph related offline analytics. These include Graphlab, Pregel and Cassowary.
 Cassowary is used for recommendations, similarity comparisons and search, ad targeting. These types of quries can run in the background and be stored for later retrieval. Also, these systems tend to load data once and then be read only. For this reason, data can be compressed much more and it can often fit into a single machine even at the scale that Twitter must work at. For example, the connections between 1/2 billion users with a combined degree of 50 can be represented efficiently in around 100GB.  using  sorted arrays as adjacency lists.   It is conceivable to handle this much data in a single machine, in memory. Computations that traverse the graph or do some kind of general pattern matching (eg queries suchs find all groups of 5 people where everyone follows everyone) across the whole graph can be done much more efficiently this way, and the graph can be reconstructed from a snapshot every few days. 

Others are for online work. These include FlockDB, used for a few different purposes at Twitter including storing who follows whom. Operations here are a more basic, and form a core component of the service. Every follow relation is stored here, persistently. These systems need to react immediately. When a user follows another, this change  should be reflected in the user list, and the follows icon immediately. New tweets from another should be delivered. Similarly, when a user blocks another the system should react immediately. This is not the case with recommendations, which can be recomputed more slowly.

Pujol, Erramilli et al \cite{little} propose an online heuristic called SPAR (for social partitioning and replication) for partitioning the kind of data such as the Twitter graph. Their aim is for the system to minimize the number of graph edges going across partitions. (needs more detail)  This system reacts to each node or edge  addition and deletion using a local, greedy heuristic. The advantages of this approach are that making a decision about where each node and its replicas live requires only limited computation and is done online. The motivation for storing adjacent nodes together is that many operations naturally require reading/modifying user tables of both neighbors.   For example, delivering a tweet t from A to B requires reading B from A's follower list and inserting $t$ into B's timeline table. If both tables are in the same node, the full operation only requires local procedure calls.  They contrast their online, greedy approach with community detection based heuristics as well as established, offline graph partitioning based heuristics such as METIS.  \emph{(note: can elaborate on this and maybe analyze the cost of running such a heuristic)}

A few approaches to partitioning a graph, especifically a social relations one, is try to infer social clusters (in the assumption that the graphs must naturally show some sort of cluster structure). These procedures tend to need a view of the full graph. Another approach is to ignore any potential social structure to partition the graph, and simply  use a graph partitioner that explicitly minimizes cut size while constraining the imbalance,  such as METIS. the MEtis heuristic has a reputation for producing good partitionings, but also is a global optimization algorithm and it is unclear how sensitive the resulting partitioning is to small changes in the input graph. 

One useful approach to generate ideas for infrascture optimization system is to exploit some of the particular features of the network or the behavior of users itself. A good example of this is the system described in Feeding Frenzy \cite{frenzy}. The problem it is aiming to solve is not related to data partitioning  but to tweet delivery. To show a user her tweet feed we can use a pull based approach, where upon request we lookup who she follows and fetch the most recent tweets to show her (thereby increasing latency for the reader) Or we can be pessimistic and pre-compute her tweet timeline (thereby reducing latency). The problem with the pre-compute strategy is that a lot of work gets wasted on computing timelines the user will never see, if the user seldom logs in. Their key insight is that by treating each user - producer pair separately (according to their production/ consumption rates) They can minimize the global amount of bandwidth required by the delivery system while keeping latency low. So they settle on a hybrid stragtegy that needs to look up information for each producer/consumer in order to optimize performance. 

\section{The abstract partitioning problem}

Data partitioning problems, offline or online, are often hard. One such basic problem is the number partitioning problem. 

\emph{input} A group of $n$  positive numbers $S = {a_1, ... a_n}$

\emph{output} A partition of the numbers into two groups $A$ and $S \ A$  such that $|\sum_{a_i \in A} a_i - \sum_{a_j \not \in A} a_j|$ is minimized.

This problem can be interpreted as a way to optimally balance the load across servers if we knew ahead of time the load $a_i$ for each task.   In that case, a scheduler could run the algorithm and then allocate tasks accordingly.   There exist several methods to generate a candidate solution to this problem in linear time as the Karmarkar Karp heuristic. 

Often, data partitioning problems also need to conisder communication costs. If doing task $a_1$  and $a_2$ involves some sort of communication, then we may want to group them in the same partition. Problems like this with an added communication cost can be ultimately modeled as the graph partitioning problems. Formally speaking, the graph partiotioning problem is the following:

\emph{input}  A graph $G = (V,E)$, a number of partitions $m$. The graph itself can have both vertex ${u_v}$ and edge weights ${w_e}$.

\emph{output} A partition of $V$ with none of the the parts is larger than  $\sum{u_v}/m$  and with the minimum amount of edgeweight across the partitions.

This problem is harder than the number partitioning problem, and it is NP-hard even if the vertex weights and edge weights are all the same. So, unlike looking for shortest paths between nodes and computing other graph quantities, there are no fast, exact algorithms for this graph problem. Like for the number partition problem, there are efficient (but not necessarily effective) heuristics such as Kernighan-Lin that try finding a good solution in $O(n^2\lg{n})$ time \cite{kernighan-lin}.  METIS \cite{metis} is an algorithm and associated implementations that aims to solve larger scale instances of these problems, and can handle parallel execution.

Graph partitioning algorithms such as these have been used for years for applicatoins such as splitting a mesh for later, processing by a parallel machine. 

All of these methods are results on the offline, static case, which suggests the online case has even less of a clean solution. One challenge is to repartition without moving too much data. This implies that running a static partitioner and then rerunning it later on is not a good solution because it offers little guarantee that the new partitioning is similar to the old. Another challenge is that as graphs increase in size, it is no longer possible to have a full view of the graph in a single machine. So a solution relying on a single memory data strucutre representing the graph is no longer feasible.  METIS is able to work with large graphs by using a graph coarsening (and subsequent restoration) heuristic, and only run a full partitioning algorithm on a smaller graph. METIS also has a relative, PARMETIS, that handles distributed graphs. 

A final challenge is to update the graph partitioning itself as we add more data.  Several heuristics for distributed, streaming graphs partitioning are proposed and evaluated  in \cite{streaming}. They also compare them to METIS  and hashing. The partitioning problem for graphs is hard but in a real system we do not need to  partition optimally. Moreover,  currently, methods such as partition by Hashing don't even attempt to minimize edge crossing.
