This is a short proposal, mostly some motivation and topic organization I'm coming up with based on information gathered from talking to Alex, Stu and RvP this week,  and  Abhi, Ryan and Stu and RvP last week in different contexts.  All those conversations helped, thanks to all of you for taking from your time to discuss this.

This is what I envision to be the structure of my MEng project/thesis:
       -design a couple sharding schemes (ie a map: key -> set<replicas> ). These make three together: one that is currently in the pipeline, the idea would be to make two others based on actual data from the graph and load skew.
       -implement them on top of the proposed (elsewhere) architecture
       -design some benchmarks, then implemenet and run.

More in detail:
-Design part:

I have some motivation for the design part, though no well thought out alternative designs. 

Right now sharding is planned  to be done by hashing an edge based on its left endpoint, ie a mapping like (A,B) -> A -> shard_id. Which guarantees edge locality by mapping all edges with the same left vertex shard to the same sharding unit.  
There are two immediate and competing, interests. One is minimizing load imbalance, the other minimizing the costs of distribution. (ie one-shard is one extreme, all-shards queries are another one). The 'optimal' scheme depends on actual skews of both the degree distribution for the graph (available in Raghavendra's data earlier this week) as well as the skew in queries to particular pieces of data, and also the relative frequencies of single edge queries (does A follow B) vs multiple edge queries (who follows A).   For example, if single edge queries get(A,B) are overwhelmingly more common than get(A,*) then locality is not as important, and higher numbers of messages across multiple nodes could be tolerated.  As another example, if a lot of individual single edge queries go to edges from the same left vertex A, then spreading them out across shards is best.  Similarly, if nodes have many many followers, then serving everyting from a single shard may hinder parallelism. All of these are questions that depend on the actual facts of the graph and queries.

There are target throughputs for different operations, as well as target performance at the 99.99 level that need to face the uneven distributions. The proposed sharding by left node seems like a good option because the 99.99 number of followers is ~25000 so roughly, if a single shard is to hold all of them, then a query of the form 'get all followers for A'  requires (2.5 e 4 edges) * (1 e 2 Bytes/edge) = 2.5 e 6 B = 2.5 MB   (x2 to allowing for some memory overhead, besides replication). Reading this sequentially from Memory takes ~ 250 usec/MB * 5 MB ~ 1 ms. Which is well within the 10 ms bounds.  But on the other hand, there are nodes in the distribution tail that right now have 10M edges, the same read delay calculation becomes 1 e 7 edges * 1 e 2 bytes/edge = 1 e 9 B = 1 GB, and allowing for some memory overhead, 2GB. ~  500 ms. If multiple Redis processes are used per node, as suggested, this may be manageable (you stil need to partition at that level). Since these are users beyond the 99.99ile in the distribution, maybe its not a big deal, on the other hand maybe peforming better for this small but influential group of users is of some value beyond simply optimizing for 99.99% of the cases would reflect. For starters, if we mean 99.99 percent of individual <requests> must be below 10ms, then the fact that queries to these particular 0.001% of <users> are slow may actually translate to violating the SLA depending on how many go to this particular group.

Going deeper in the reasoning of the previous paragraph. There are potential interdepencies in 'skew' that I have not considered but could be maybe worth taking a look at in data. Hypothetically, there could be dependencies in the the degree frequency skew and the query frequency for a particular or node edge. For example, the users with the most followers are also likely to be the ones growing the fastest, so insert queries to their nodes may be more frequent. It may also be the case that users with more followers tend to tweet more often, so queries asking for all their followers are executed more often. While these users may be a smaller fraction, the overall percentage of requests made for them is probably larger than 0.001%.  There are a few more observations of these types that could be investigated on the actual use logs, and that could be relevant to any design, and a potential part of the project. It could also be these things are just not significant, I'm not sure. While the complexity of this reasoning, the system designs resulting from it need not be complex.

Another issue is that the eventual strategy to be used for online load balancing (if any) is coupled with the choice of sharding method. For example, if we are range partitioning, then we may need to split the ranges as keys are added, but if we are hashing by edge the partition will tend to be more even naturally. Other arbitrary partitioning methods, such as the 'clustered' consistent hashing method that was proposed earlier this week have different profiles in terms of the work to be done when a single node is added (ie, many nodes may be involved in communicating state to the new one, maybe 1, maybe O(1) but hopefully not O(N) where n is the number of nodes). The MEng project can include comparison of sharding methods when the dynamic balancing appropriate to them is also included, but I am not fully clear on the implementation difficulties here.  If we consider sharding functions that are expressable as a key ->  shard_i followed by a shard_id -> nodes, then there are two levels of things that can change, one about when a key -> shard_id map changes (eg splitting in a range partition) and other purely based on shard_id ->  nodes. This perspective allows us to consider different combinations of choices for the first and second.  Ie we can use either hash or range for the key -> shard_id part, and then have the same strategy for the shard_id -> node. 
 Some methods like consistent hashing can be thought of this way, but they are also representable directly as key -> node_id.  The size of the representation of this mapping is a limiting factor. In the case of consistent hashing there is only the hash function + the list of live nodes, but in the case of more arbitrary mappings you need some kind of lookup structure for example.

Just like perhaps sharding by single node maybe be all that is needed, a lot of these considerations may in the end not matter, in the sense that perhaps the 'extreme' but simple solutions at the end of the design dimensions are really the best anyway. For a thesis it is enough to have done the exploration, for Twitter I'd like to have at least something that informs your thinking. 

Though I do not have a particular sharding scheme well thought out, here are a few I have in mind: 1) Range partitioning (it is not expected to be done at a first iteration by Stu, so thinking going into may be useful in a timely way) 2)  Hash partitioning as discussed, except for a select few thousands of nodes that cut above a certain treshold, which get split into more than one unit. This way reads of the 'all followers of A' query can achieve more parallelism (perhaps the one provided by multicore is enough, perhaps the replicated units can be used for this already).   3) partitioning that takes into account consistency constraints between forward edges and back edges (A follows B iff B is followed by A) to try and place these things together. As I understand, these constraints are going to be satisfied 'eventually' after updating the first edge, but even then, the inconsistency window can be minimized if the sharding helps.

Lastly, besides the the two queries I keep the most in my mind:  get(A,B) and get(A,*), the are other types of queries such as "get(A,*) Union get(B,*)" or intersections. I have to think about the benefits possible in these areas for each scheme. These operations also would benefit from parallelism in some cases (large edge sets), and if data is in different shards then that helps parellism. Union can be efficiently parallelized so for large sets of edges, it may helpful to split. 

-Implementation:

Right now there is one proposed sharding library architecture Stu is working on, using Zookeeper in a way similar to the PNUTS tablet manager, and using the API servers similarly to the PNUTS routers.  I'd hope to use these systems, but I am yet to become familiar with the restrictions imposed by ZooKeeper to store state.  As of right now I understand data stored zK is meant to be laid out as a tree structure with some node size and query limitations. 

-Benchmarks:

The goal here is to measure how much better or worse these perform, after fixing the number of machines, based on the empirical patterns found earlier in the design part.  Workloads could be derived from real operation data or hypothetical data with skews like done in the PNUTS paper.  

Bibliography (that immediately comes to mind, will keep growing)
-PNUTS paper
-Fine Grained partitioning paper (by my school advisor)
-Ruhl and Karger paper on online rebalancing.

Notes:
-Over the next days I will keep making this more of an actual proposal (ie with more of my own input) rather than a background summary, but still need to increase background.
-Let me know what things about this are wrong.
-I'll keep trying to make a more detailed project description and figure out how much I can actually do given the time (here to end of January)
-I am officially registered so it was approved (woohoo)
